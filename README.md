# NLP Models
 **This repository has GPT-2 pre-training and sequence generation implementation in tensorflow 2.0, **

**Requirements**

*  python >= 3.6
*  setuptools==41.0.1
*  ftfy==5.6
*  tqdm==4.32.1
*  Click==7.0
*  sentencepiece==0.1.83
*  tensorflow-gpu==2.3.0
*  numpy==1.16.4

**References:**

* ["Openai/gpt-2"](https://github.com/openai/gpt-2)
* ["Huggingface pytorch-transformers"](https://github.com/huggingface/pytorch-transformers)
* ["Tensorflow Transformers"](https://www.tensorflow.org/beta/tutorials/text/transformer)
* ["The Illustrated GPT-2 "](https://jalammar.github.io/illustrated-gpt2/)

* <p align="center">
Computation Graph of GPT-2 Model.
</p>

<div align="center">
<img src="https://raw.githubusercontent.com/akanyaani/gpt-2-tensorflow2.0/master/images/GPT-2_Decoder.jpg" alt="Decoder Graph" height="750" width="700"/>
<div>
<br />          
<div align="center">
<img src="https://raw.githubusercontent.com/akanyaani/gpt-2-tensorflow2.0/master/images/GPT-2_Graph.jpg" alt="GPT-2_Graph" height="750" width="700"/>
<div>
